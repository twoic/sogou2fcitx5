#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import argparse
from pathlib import Path
from sougou_downloader import SougouSpider

# ==== æ‰€æœ‰å¯ç”¨åˆ†ç±» ====
ALL_CATEGORIES = {
    "åŸå¸‚ä¿¡æ¯": 167,
    "è‡ªç„¶ç§‘å­¦": 1,
    "ç¤¾ä¼šç§‘å­¦": 76,
    "å·¥ç¨‹åº”ç”¨": 96,
    "å†œæ—æ¸”ç•œ": 127,
    "åŒ»å­¦åŒ»è¯": 132,
    "ç”µå­æ¸¸æˆ": 436,
    "è‰ºæœ¯è®¾è®¡": 154,
    "ç”Ÿæ´»ç™¾ç§‘": 389,
    "è¿åŠ¨ä¼‘é—²": 367,
    "äººæ–‡ç§‘å­¦": 31,
    "å¨±ä¹ä¼‘é—²": 403,
}

# ==== è§£æå‘½ä»¤è¡Œå‚æ•° ====
parser = argparse.ArgumentParser(description="æœç‹—æ‹¼éŸ³è¯åº“ä¸‹è½½å·¥å…·")
parser.add_argument("--recommended", action="store_true", help="ä»…ä¸‹è½½å®˜æ–¹æ¨èè¯åº“")
parser.add_argument("--workers", type=int, default=None, help="æœ€å¤§çº¿ç¨‹æ•°ï¼ˆé»˜è®¤æ™ºèƒ½åˆ†é…ï¼‰")
parser.add_argument(
    "--categories",
    type=int,
    nargs="*",
    help="æŒ‡å®šè¦ä¸‹è½½çš„åˆ†ç±» IDï¼ˆå¤šä¸ªç”¨ç©ºæ ¼åˆ†éš”ï¼Œå¦‚ï¼š1 132 436ï¼‰",
)
args = parser.parse_args()

# ==== åŸºç¡€è·¯å¾„ ====
base_path = Path(__file__).resolve().parent
save_path = base_path / "sogou_scel"

# ==== ç­›é€‰åˆ†ç±» ====
if args.categories:
    categories = {k: v for k, v in ALL_CATEGORIES.items() if v in args.categories}
    if not categories:
        print("âŒ æ— æ•ˆçš„åˆ†ç±» IDï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚")
        exit(1)
else:
    categories = ALL_CATEGORIES

# ==== åˆå§‹åŒ–çˆ¬è™« ====
spider = SougouSpider(max_workers=args.workers)

# ==== ä¸»æ‰§è¡Œé€»è¾‘ ====
for cate_name, cate_id in categories.items():
    cate_url = f"https://pinyin.sogou.com/dict/cate/index/{cate_id}"
    cate_dir = save_path / str(cate_id)
    cate_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ—‚ï¸ æ­£åœ¨å¤„ç†å¤§ç±»ï¼š{cate_name}ï¼ˆID: {cate_id}ï¼‰")

    # è·å–å­ç±»åˆ—è¡¨
    resp = spider.get_html(cate_url)
    sub_cates = (
        spider.get_category_type1(resp)
        if str(cate_id) == "167"
        else spider.get_category_type2(resp)
    )

    for sub_name, sub_url in sub_cates.items():
        sub_dir = cate_dir / sub_name
        sub_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nğŸ“ å­ç±»ï¼š{sub_name}")
        resp = spider.get_html(sub_url)
        page_count = spider.get_page_count(resp)

        # æ”¶é›†ä¸‹è½½é“¾æ¥
        all_links = {}
        for page in range(1, page_count + 1):
            page_url = f"{sub_url}/default/{page}"
            resp = spider.get_html(page_url)
            download_links = spider.get_download_list(resp)

            if args.recommended:
                download_links = {
                    k: v for k, v in download_links.items() if "å®˜æ–¹æ¨è" in k
                }

            all_links.update(download_links)

        if all_links:
            print(f"ğŸ“¦ å…± {len(all_links)} ä¸ªè¯åº“å¾…ä¸‹è½½")
            spider.download_dicts(all_links, sub_dir)

print("\nâœ… æ‰€æœ‰è¯åº“ä¸‹è½½å®Œæˆï¼")
